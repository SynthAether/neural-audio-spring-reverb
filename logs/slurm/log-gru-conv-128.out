Using device: cuda:0
Using configuration gru-conv-4-128
Configuration name: gru-conv-4-128
Parameters: 398.338 k
Using losses: STFTLoss and SmoothL1Loss
Total dataset samples: 690
Training model for 1000 epochs, current epoch 0
Epoch 0: Loss improved from  inf to 1.750499 - > Saving model
Epoch 2: Loss improved from 1.750499 to 1.690290 - > Saving model
Epoch 3: Loss improved from 1.690290 to 1.604377 - > Saving model
Epoch 4: Loss improved from 1.604377 to 1.411539 - > Saving model
Epoch 5: Loss improved from 1.411539 to 1.305587 - > Saving model
Epoch 6: Loss improved from 1.305587 to 1.294498 - > Saving model
Epoch 7: Loss improved from 1.294498 to 1.280633 - > Saving model
Epoch 8: Loss improved from 1.280633 to 1.211095 - > Saving model
Epoch 9: Loss improved from 1.211095 to 1.172156 - > Saving model
Epoch 13: Loss improved from 1.172156 to 1.143917 - > Saving model
Epoch 17: Loss improved from 1.143917 to 1.138210 - > Saving model
Epoch 20: Loss improved from 1.138210 to 1.128911 - > Saving model
Epoch 21: Loss improved from 1.128911 to 1.115763 - > Saving model
Epoch 25: Loss improved from 1.115763 to 1.093280 - > Saving model
Epoch 27: Loss improved from 1.093280 to 1.089449 - > Saving model
Epoch 29: Loss improved from 1.089449 to 1.075959 - > Saving model
Epoch 37: Loss improved from 1.075959 to 1.053295 - > Saving model
Epoch 45: Loss improved from 1.053295 to 1.046169 - > Saving model
Epoch 54: Loss improved from 1.046169 to 1.035795 - > Saving model
Epoch 57: Loss improved from 1.035795 to 1.033693 - > Saving model
Epoch 62: Loss improved from 1.033693 to 1.029774 - > Saving model
Epoch 00074: reducing learning rate of group 0 to 1.0000e-04.
Epoch 74: Loss improved from 1.029774 to 1.028610 - > Saving model
Epoch 75: Loss improved from 1.028610 to 1.024936 - > Saving model
Epoch 76: Loss improved from 1.024936 to 1.024299 - > Saving model
Epoch 77: Loss improved from 1.024299 to 1.022952 - > Saving model
Epoch 79: Loss improved from 1.022952 to 1.022915 - > Saving model
Epoch 81: Loss improved from 1.022915 to 1.022353 - > Saving model
Epoch 82: Loss improved from 1.022353 to 1.021596 - > Saving model
Epoch 00094: reducing learning rate of group 0 to 1.0000e-05.
Epoch 94: Loss improved from 1.021596 to 1.019649 - > Saving model
Epoch 00106: reducing learning rate of group 0 to 1.0000e-06.
Epoch 00117: reducing learning rate of group 0 to 1.0000e-07.
Epoch 00128: reducing learning rate of group 0 to 1.0000e-08.
Epoch 194: Loss did not improve for 100 epochs, stopping training
Epoch 194, final validation loss: 1.0244233141774717
