{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Audio Features with Essentia Streaming\n",
    "\n",
    "Essentia is an open-source C++ library for audio analysis and audio-based music information retrieval.\n",
    "Documentation: http://essentia.upf.edu/documentation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import essentia\n",
    "import essentia.standard as esstd\n",
    "import essentia.streaming as esstr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from fnmatch import fnmatch\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data.egfxset import load_egfxset\n",
    "from src.data.springset import load_springset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../data/raw/')\n",
    "MODELS_DIR = Path('../models/')\n",
    "RESULTS_DIR = Path('../data/features/')\n",
    "PLOTS_DIR = Path('../docs/plots/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_basic_features_essentia(data, sample_rate):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    pool = essentia.Pool()\n",
    "\n",
    "    # Instantiate the algorithms\n",
    "    # loader = esstr.MonoLoader(filename=file, sampleRate=sample_rate)\n",
    "    loader = esstr.VectorInput(data)\n",
    "    fcut = esstr.FrameCutter(frameSize=2048, hopSize=1024)\n",
    "    w = esstr.Windowing(type='hann')\n",
    "    spec = esstr.Spectrum(size=2048)\n",
    "    \n",
    "    gain = esstr.ReplayGain(sampleRate=sample_rate)\n",
    "    leq = esstr.Leq()\n",
    "    loudness = esstr.Loudness()\n",
    "\n",
    "    zero_crossing_rate = esstr.ZeroCrossingRate()\n",
    "    \n",
    "    centroid = esstr.SpectralCentroidTime(sampleRate=sample_rate)\n",
    "    pitch = esstr.PitchYin(sampleRate=sample_rate)\n",
    "\n",
    "    # Connect the algorithms\n",
    "    loader.data >> fcut.signal\n",
    "    fcut.frame >> centroid.array\n",
    "    fcut.frame >> loudness.signal\n",
    "\n",
    "    fcut.frame >> zero_crossing_rate.signal\n",
    "    \n",
    "    fcut.frame >> pitch.signal\n",
    "   \n",
    "    loader.data >> gain.signal\n",
    "    loader.data >> leq.signal\n",
    "    \n",
    "    # Create a pool and output algorithms\n",
    "    gain.replayGain >> (pool, 'gain')\n",
    "    leq.leq >> (pool, 'leq')\n",
    "    loudness.loudness >> (pool, 'loudness')\n",
    "    \n",
    "    zero_crossing_rate.zeroCrossingRate >> (pool, 'zcr')\n",
    "    \n",
    "    centroid.centroid >> (pool, 'centroid')\n",
    "\n",
    "    pitch.pitch >> (pool, 'pitch')\n",
    "    pitch.pitchConfidence >> (pool, 'confidence')\n",
    "\n",
    "    # Run the network\n",
    "    essentia.run(loader)\n",
    "\n",
    "    aggrpool = esstd.PoolAggregator(defaultStats = [\"mean\"])(pool)    \n",
    "    descriptors = aggrpool.descriptorNames()\n",
    "    \n",
    "    # for feature in ['gain', 'leq', 'loudness.mean', 'centroid.mean', 'pitch.mean', 'confidence.mean']:\n",
    "    #     features[feature].append(np.array(aggrpool[feature]).flatten()[0])\n",
    "    for feature in ['gain', 'leq', 'loudness.mean', 'zcr.mean', 'centroid.mean', 'pitch.mean', 'confidence.mean']:\n",
    "        value = np.array(aggrpool[feature]).flatten()[0]\n",
    "        if feature not in features:\n",
    "            features[feature] = []\n",
    "            features[feature].append(value)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger set of features\n",
    "\n",
    "\n",
    "to get help:\n",
    "```python\n",
    "help(esstr.SNR())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(esstr.SNR())\n",
    "print(help(esstr.LowLevelSpectralExtractor()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_more_features_essentia(data, sample_rate):\n",
    "    \"\"\"Extract various audio features using Essentia's streaming mode.\"\"\"\n",
    "    \n",
    "    frame_size = 1024\n",
    "    # noise_threshold = -90\n",
    "\n",
    "    features = {}\n",
    "    pool = essentia.Pool()\n",
    "\n",
    "    # Instantiate the algorithms\n",
    "    loader = esstr.VectorInput(data)\n",
    "    fcut = esstr.FrameCutter(frameSize=frame_size, hopSize=512)\n",
    "    w = esstr.Windowing()\n",
    "    spec = esstr.Spectrum(size=frame_size)\n",
    "\n",
    "    gain = esstr.ReplayGain(sampleRate=sample_rate)\n",
    "    leq = esstr.Leq()\n",
    "    loudness = esstr.Loudness()\n",
    "    zero_crossing_rate = esstr.ZeroCrossingRate()\n",
    "    centroid = esstr.SpectralCentroidTime(sampleRate=sample_rate)\n",
    "    pitch = esstr.PitchYin(sampleRate=sample_rate, frameSize=frame_size)\n",
    "    \n",
    "    # New features' algorithms\n",
    "    flatness = esstr.FlatnessDB()\n",
    "    hfc = esstr.HFC(sampleRate=sample_rate)\n",
    "    # snr = esstr.SNR(sampleRate=sample_rate, frameSize=frame_size, noiseThreshold=noise_threshold)\n",
    "\n",
    "    # Connect the algorithms\n",
    "    loader.data >> fcut.signal\n",
    "    loader.data >> gain.signal\n",
    "    loader.data >> leq.signal\n",
    "\n",
    "    fcut.frame >> centroid.array\n",
    "    fcut.frame >> loudness.signal\n",
    "    fcut.frame >> zero_crossing_rate.signal\n",
    "    fcut.frame >> pitch.signal\n",
    "    # fcut.frame >> snr.frame\n",
    "    \n",
    "    fcut.frame >> w.frame\n",
    "    w.frame >> spec.frame\n",
    "\n",
    "    spec.spectrum >> flatness.array\n",
    "    spec.spectrum >> hfc.spectrum\n",
    "    \n",
    "    # Add the features to the pool\n",
    "    gain.replayGain >> (pool, 'gain')\n",
    "    leq.leq >> (pool, 'leq')\n",
    "    loudness.loudness >> (pool, 'loudness')\n",
    "    zero_crossing_rate.zeroCrossingRate >> (pool, 'zcr')\n",
    "    centroid.centroid >> (pool, 'centroid')\n",
    "    pitch.pitch >> (pool, 'pitch')\n",
    "    pitch.pitchConfidence >> (pool, 'confidence')\n",
    "\n",
    "    # snr.instantSNR >> None\n",
    "    # snr.averagedSNR >> None\n",
    "    # snr.spectralSNR >> (pool, 'spectralSNR')\n",
    "    \n",
    "    flatness.flatnessDB >> (pool, 'flatness')\n",
    "    hfc.hfc >> (pool, 'hfc')\n",
    "    \n",
    "    # Run the network\n",
    "    essentia.run(loader)\n",
    "\n",
    "    aggrpool = esstd.PoolAggregator(defaultStats = [\"mean\"])(pool)    \n",
    "\n",
    "    # Extract features\n",
    "    for feature in ['gain', 'leq', 'loudness.mean', 'zcr.mean', 'pitch.mean', 'confidence.mean', 'flatness.mean', 'hfc.mean']:\n",
    "        value = np.array(aggrpool[feature]).flatten()[0]\n",
    "        \n",
    "        # Remove the .mean suffix for storage\n",
    "        feature_name = feature.replace('.mean', '')\n",
    "\n",
    "        if feature_name not in features:\n",
    "            features[feature_name] = []\n",
    "        features[feature_name].append(value)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EGFxSet\n",
    "\n",
    "This may take long time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 48000\n",
    "train_loader, valid_loader, test_loader = load_egfxset(datadir=DATA_DIR, batch_size=1, train_ratio=0.50, valid_ratio=0.25, test_ratio=0.25, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EGFxSet: train set\n",
    "\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for idx, (input, target) in enumerate(train_loader):\n",
    "    # Convert tensor to numpy and ensure dtype and shape\n",
    "    input_np = input.numpy().squeeze().astype(np.float32)\n",
    "    target_np = target.numpy().squeeze().astype(np.float32)\n",
    "\n",
    "    # Extract features from the dry signal\n",
    "    x_features = extract_more_features_essentia(input_np, sample_rate)\n",
    "    y_features = extract_more_features_essentia(target_np, sample_rate)\n",
    "\n",
    "    x_features = {f'{key}': value for key, value in x_features.items()}\n",
    "    y_features = {f'{key}': value for key, value in y_features.items()}\n",
    "    \n",
    "    data_x.append({'idx': idx, **x_features})\n",
    "    data_y.append({'idx': idx, **y_features})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_x = pd.DataFrame(data_x)\n",
    "df_y = pd.DataFrame(data_y)\n",
    "\n",
    "# Save DataFrame as a .json file\n",
    "df_x.to_json(os.path.join(RESULTS_DIR, 'egfxset_x_train.json'), orient='records', lines=True)\n",
    "df_y.to_json(os.path.join(RESULTS_DIR, 'egfxset_y_train.json'), orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EGFxSet: validation set\n",
    "\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for idx, (input, target) in enumerate(valid_loader):\n",
    "    # Convert tensor to numpy and ensure dtype and shape\n",
    "    input_np = input.numpy().squeeze().astype(np.float32)\n",
    "    target_np = target.numpy().squeeze().astype(np.float32)\n",
    "\n",
    "    # Extract features from the dry signal\n",
    "    x_features = extract_more_features_essentia(input_np, sample_rate)\n",
    "    y_features = extract_more_features_essentia(target_np, sample_rate)\n",
    "\n",
    "    x_features = {f'{key}': value for key, value in x_features.items()}\n",
    "    y_features = {f'{key}': value for key, value in y_features.items()}\n",
    "    \n",
    "    data_x.append({'idx': idx, **x_features})\n",
    "    data_y.append({'idx': idx, **y_features})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_x = pd.DataFrame(data_x)\n",
    "df_y = pd.DataFrame(data_y)\n",
    "\n",
    "# Save DataFrame as a .json file\n",
    "df_x.to_json(os.path.join(RESULTS_DIR, 'egfxset_x_valid.json'), orient='records', lines=True)\n",
    "df_y.to_json(os.path.join(RESULTS_DIR, 'egfxset_y_valid.json'), orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EGFxSet: test set\n",
    "\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for idx, (input, target) in enumerate(test_loader):\n",
    "    # Convert tensor to numpy and ensure dtype and shape\n",
    "    input_np = input.numpy().squeeze().astype(np.float32)\n",
    "    target_np = target.numpy().squeeze().astype(np.float32)\n",
    "\n",
    "    # Extract features from the dry signal\n",
    "    x_features = extract_more_features_essentia(input_np, sample_rate)\n",
    "    y_features = extract_more_features_essentia(target_np, sample_rate)\n",
    "\n",
    "    x_features = {f'{key}': value for key, value in x_features.items()}\n",
    "    y_features = {f'{key}': value for key, value in y_features.items()}\n",
    "    \n",
    "    data_x.append({'idx': idx, **x_features})\n",
    "    data_y.append({'idx': idx, **y_features})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_x = pd.DataFrame(data_x)\n",
    "df_y = pd.DataFrame(data_y)\n",
    "\n",
    "# Save DataFrame as a .json file\n",
    "df_x.to_json(os.path.join(RESULTS_DIR, 'egfxset_x_test.json'), orient='records', lines=True)\n",
    "df_y.to_json(os.path.join(RESULTS_DIR, 'egfxset_y_test.json'), orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpringSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 files in ../data/raw/spring\n",
      "Using dry_train.h5 and wet_train.h5 for train split.\n",
      "Found 4 files in ../data/raw/spring\n",
      "Using dry_val_test.h5 and wet_val_test.h5 for test split.\n"
     ]
    }
   ],
   "source": [
    "sample_rate = 16000\n",
    "\n",
    "train_loader, valid_loader, test_loader = load_springset(datadir=DATA_DIR, batch_size=1, train_ratio=0.70, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpringSet: train set\n",
    "\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for idx, (input, target) in enumerate(train_loader):\n",
    "    # Convert tensor to numpy and ensure dtype and shape\n",
    "    input_np = input.numpy().squeeze().astype(np.float32)\n",
    "    target_np = target.numpy().squeeze().astype(np.float32)\n",
    "\n",
    "    # Extract features from the dry signal\n",
    "    x_features = extract_more_features_essentia(input_np, sample_rate)\n",
    "    y_features = extract_more_features_essentia(target_np, sample_rate)\n",
    "\n",
    "    x_features = {f'{key}': value for key, value in x_features.items()}\n",
    "    y_features = {f'{key}': value for key, value in y_features.items()}\n",
    "    \n",
    "    data_x.append({'idx': idx, **x_features})\n",
    "    data_y.append({'idx': idx, **y_features})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_x = pd.DataFrame(data_x)\n",
    "df_y = pd.DataFrame(data_y)\n",
    "\n",
    "# Save DataFrame as a .json file\n",
    "df_x.to_json(os.path.join(RESULTS_DIR, 'springset_x_train.json'), orient='records', lines=True)\n",
    "df_y.to_json(os.path.join(RESULTS_DIR, 'springset_y_train.json'), orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpringSet: validation set\n",
    "\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for idx, (input, target) in enumerate(valid_loader):\n",
    "    # Convert tensor to numpy and ensure dtype and shape\n",
    "    input_np = input.numpy().squeeze().astype(np.float32)\n",
    "    target_np = target.numpy().squeeze().astype(np.float32)\n",
    "\n",
    "    # Extract features from the dry signal\n",
    "    x_features = extract_more_features_essentia(input_np, sample_rate)\n",
    "    y_features = extract_more_features_essentia(target_np, sample_rate)\n",
    "\n",
    "    x_features = {f'{key}': value for key, value in x_features.items()}\n",
    "    y_features = {f'{key}': value for key, value in y_features.items()}\n",
    "    \n",
    "    data_x.append({'idx': idx, **x_features})\n",
    "    data_y.append({'idx': idx, **y_features})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_x = pd.DataFrame(data_x)\n",
    "df_y = pd.DataFrame(data_y)\n",
    "\n",
    "# Save DataFrame as a .json file\n",
    "df_x.to_json(os.path.join(RESULTS_DIR, 'springset_x_valid.json'), orient='records', lines=True)\n",
    "df_y.to_json(os.path.join(RESULTS_DIR, 'springset_y_valid.json'), orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpringSet: test set\n",
    "\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for idx, (input, target) in enumerate(test_loader):\n",
    "    # Convert tensor to numpy and ensure dtype and shape\n",
    "    input_np = input.numpy().squeeze().astype(np.float32)\n",
    "    target_np = target.numpy().squeeze().astype(np.float32)\n",
    "\n",
    "    # Extract features from the dry signal\n",
    "    x_features = extract_more_features_essentia(input_np, sample_rate)\n",
    "    y_features = extract_more_features_essentia(target_np, sample_rate)\n",
    "\n",
    "    x_features = {f'{key}': value for key, value in x_features.items()}\n",
    "    y_features = {f'{key}': value for key, value in y_features.items()}\n",
    "    \n",
    "    data_x.append({'idx': idx, **x_features})\n",
    "    data_y.append({'idx': idx, **y_features})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_x = pd.DataFrame(data_x)\n",
    "df_y = pd.DataFrame(data_y)\n",
    "\n",
    "# Save DataFrame as a .json file\n",
    "df_x.to_json(os.path.join(RESULTS_DIR, 'springset_x_test.json'), orient='records', lines=True)\n",
    "df_y.to_json(os.path.join(RESULTS_DIR, 'springset_y_test.json'), orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
