{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Audio Samples\n",
    "\n",
    "Use this notebook to plot waveforms and spectrograms of the obtained audio samples from training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy import fft, signal\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.dpi'] = 200            # Set figure quality\n",
    "plt.rcParams['figure.figsize'] = (8, 5)     # Set default figure size\n",
    "\n",
    "plt.rcParams['lines.linewidth'] = 0.4     # Set the default linewidth  \n",
    "plt.rcParams['axes.grid'] = True           # Enable grid lines\n",
    "plt.rcParams['font.size'] = 10             # Set the default linewidth\n",
    "\n",
    "%matplotlib inline\n",
    "# mpl.rcParams.keys()                       # List all the parameters\n",
    "# mpl.rcParams.update(mpl.rcParamsDefault)  # Restore defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SAMPLES = Path(\"../audio/train\")\n",
    "EVAL_SAMPLES = Path(\"../audio/eval\")\n",
    "PLOTS_DIR = Path(\"../docs/plots/eval/\")\n",
    "\n",
    "start_idx = 0.05\n",
    "end_idx = 0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dc_offset(pred, target):\n",
    "    pred_offset = pred.mean()\n",
    "    target_offset = target.mean()\n",
    "    dc_offset = pred_offset - target_offset\n",
    "\n",
    "    # Smallest quantization step for 16-bit audio\n",
    "    quantization_step_16bit = 1 / 32767.0\n",
    "\n",
    "    # Check if the absolute value of the DC offset is smaller or larger than the quantization step\n",
    "    if abs(dc_offset) < quantization_step_16bit:\n",
    "        print(\"The DC offset is smaller than the smallest quantization step for 16-bit audio.\")\n",
    "    else:\n",
    "        print(\"The DC offset is larger than the smallest quantization step for 16-bit audio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_overlap_waveforms(output, target, sample_rate, start_idx, end_idx, title):    \n",
    "    \"\"\"Plots the output and target waveforms overlapped.\"\"\"\n",
    "    \n",
    "    fs = sample_rate\n",
    "    output = output[int(sample_rate * start_idx):int(sample_rate * end_idx)]\n",
    "    target = target[int(sample_rate * start_idx):int(sample_rate * end_idx)]\n",
    " \n",
    "    T = 1 / fs                              # sampling interval\n",
    "    t = np.arange(start_idx,end_idx, T)     # Time vector\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(t, output, alpha=0.8, label=\"Model\")\n",
    "    plt.plot(t, target, alpha=0.8, label=\"Target\")\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_overlap_time_freq(output, target, sample_rate, start_idx, end_idx, title):\n",
    "\n",
    "    output = output[int(sample_rate * start_idx):int(sample_rate * end_idx)]\n",
    "    target = target[int(sample_rate * start_idx):int(sample_rate * end_idx)]\n",
    "    \n",
    "    fs = sample_rate\n",
    "    T = 1 / fs                              # sampling interval\n",
    "    t = np.arange(start_idx,end_idx, T)     # Time vector\n",
    "    N = len(output)                         # FFT size\n",
    "\n",
    "    # Compute FFT with the window \n",
    "    w = signal.get_window('blackmanharris', N)    # Window function\n",
    "    output_fft = fft.rfft(output * w) / N   # FFT normalized by the FFT size \n",
    "    target_fft = fft.rfft(target * w) / N   # FFT normalized by the FFT size\n",
    "    \n",
    "    output_fft = 20 * np.log10(np.abs(output_fft[:N//2]))\n",
    "    target_fft = 20 * np.log10(np.abs(target_fft[:N//2]))\n",
    "\n",
    "    xf = fft.rfftfreq(N, T)[:N//2]          # Calculate the frequencies\n",
    "\n",
    "    fig,axs = plt.subplots(2,1)\n",
    "    fig.suptitle(title)\n",
    "    plt.sca(axs[0])\n",
    "    plt.plot(t,output, alpha=0.6, label='Output')\n",
    "    plt.plot(t,target, alpha=0.6, label='Target')\n",
    "    plt.xlim(t[0],t[-1])\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.sca(axs[1])\n",
    "    plt.semilogx(xf[:N//2], output_fft[:N//2], alpha=0.6,label='Output')\n",
    "    plt.semilogx(xf[:N//2], target_fft[:N//2], alpha=0.6,label='Target')\n",
    "    plt.xlim(50, 16000)\n",
    "    plt.ylim(-110, -20)\n",
    "    plt.xlabel('Frequency [Hz]')\n",
    "    plt.ylabel('Magnitude [dB]')\n",
    "    \n",
    "    # Set x-axis ticks\n",
    "    custom_ticks = [20, 100, 1000, 10000]\n",
    "    plt.xticks(custom_ticks, custom_ticks)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(directory_path, start_idx, end_idx):\n",
    "    \"\"\"\n",
    "    Walk through directory_path, and for each pair of pred_{some_model_name}.wav and targ_{same_model_name}.wav, \n",
    "    load them using torchaudio and call overlap_waveforms and two_spectrograms_difference functions.\n",
    "    \"\"\"\n",
    "    directory_path = Path(directory_path)\n",
    "    output_files = [f for f in directory_path.iterdir() if f.name.startswith(\"pred\") and f.suffix == \".wav\"]\n",
    "    \n",
    "    for out_file in output_files:\n",
    "        # Construct the expected name for the target file by replacing \"pred\" with \"targ\"\n",
    "        tgt_file_name = out_file.name.replace(\"pred\", \"target\")\n",
    "        tgt_file = directory_path / tgt_file_name\n",
    "        \n",
    "        if tgt_file.exists():\n",
    "            # Load the audio files using torchaudio\n",
    "            output, sample_rate = torchaudio.load(out_file)\n",
    "            target, _ = torchaudio.load(tgt_file)\n",
    "            \n",
    "            output = output.view(-1).numpy()\n",
    "            target = target.view(-1).numpy()\n",
    "\n",
    "            title = out_file.stem  # Use the original stem of the prediction file as the title\n",
    "            title = title[5:]\n",
    "            # Call the plotting functions\n",
    "            # plot_overlap_waveform(output, target, sample_rate, start_idx, end_idx, title)\n",
    "            plot_overlap_time_freq(output, target, sample_rate, start_idx, end_idx, title)\n",
    "        else:\n",
    "            print(f\"Matching target file for {out_file} not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_directory(EVAL_SAMPLES, start_idx, end_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
