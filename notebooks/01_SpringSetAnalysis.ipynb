{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R8F3RwyzNY0Z"
      },
      "source": [
        "# SpringSet: numerical and visual analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vqu3t1D8_ePE"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('../')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "import essentia\n",
        "import essentia.standard as es\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# plt.rcParams['figure.figsize'] = [10, 6]\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uMwmEk9jQsV",
        "outputId": "8e7aaad0-c62b-49b2-b00b-ee332a2fd6a0"
      },
      "outputs": [],
      "source": [
        "# Setting up the directories and the sample rate\n",
        "\n",
        "DATA_DIR = Path('../data/raw/')\n",
        "MODELS_DIR = Path('../models/')\n",
        "RESULTS_DIR = Path('../data/features/')\n",
        "PLOTS_DIR = Path('../docs/plots/')\n",
        "\n",
        "sample_rate = 16000\n",
        "spring_dir = DATA_DIR / 'springset/' "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List and read the files in the dataset folder\n",
        "\n",
        "data_dict = {}\n",
        "\n",
        "for file in os.listdir(spring_dir):\n",
        "    if file.endswith(('.h5')):\n",
        "        file_path = os.path.join(spring_dir, file)\n",
        "        with h5py.File(os.path.join(spring_dir, file), 'r') as f_handler:\n",
        "            key = list(f_handler.keys())[0]\n",
        "            data = f_handler[key]\n",
        "            print(f\"File: {file}\")\n",
        "            print(f\"Keys: {key}\")\n",
        "            if len(data.attrs) > 0:\n",
        "                print(\"Metadata attributes found:\")\n",
        "            for attr_name, attr_value in data.attrs.items():\n",
        "                print(f\"{attr_name}: {attr_value}\")\n",
        "            else:\n",
        "                print(\"No metadata attributes found.\")\n",
        "            \n",
        "            print(f\"Type of ['{key}']: {type(data[:])}\")\n",
        "            print(f\"Data shape: {data.shape}\")\n",
        "            print(f\"Data type: {data.dtype}\")\n",
        "    \n",
        "            data_dict[key] = data[:]  \n",
        "\n",
        "            print(\"\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***The data is organized in the following way: `(index, samples, channels)`***\n",
        "\n",
        "No attributes have been found in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load each part of the dataset into a variable\n",
        "X_train = data_dict['Xtrain']\n",
        "Y_train = data_dict['Ytrain_0']\n",
        "X_test = data_dict['Xvalidation']\n",
        "Y_test = data_dict['Yvalidation_0']\n",
        "\n",
        "# Print the shape of each part of the dataset\n",
        "print('Xtrain shape:', X_train.shape)\n",
        "print('Ytrain shape:', Y_train.shape)\n",
        "print('Xtest shape:', X_test.shape)\n",
        "print('Ytest shape:', Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Compute the stats of each file and the whole dataset ### FROM FILES NOT ARRAYS !!!\n",
        "\n",
        "def compute_stats_of_file(data):\n",
        "    min_val = np.min(data)\n",
        "    max_val = np.max(data)\n",
        "    mean_val = np.mean(data)\n",
        "    std_val = np.std(data)\n",
        "    return min_val, max_val, mean_val, std_val\n",
        "\n",
        "# Iterate over each key-value pair in data_dict\n",
        "for key, value in data_dict.items():\n",
        "    stats = compute_stats_of_file(value)\n",
        "\n",
        "    print(f\"{key}: Min: {stats[0]:.3f}, Max: {stats[1]:.3f}, Mean: {stats[2]:.3f}, Std: {stats[3]:.3f}\")\n",
        "\n",
        "# Concatenate all arrays into a single one\n",
        "all_data = np.concatenate(list(data_dict.values()))\n",
        "\n",
        "# Compute the stats\n",
        "global_stats = compute_stats_of_file(all_data)\n",
        "\n",
        "print(f\"Global: Min: {global_stats[0]:.3f}, Max: {global_stats[1]:.3f}, Mean: {global_stats[2]:.3f}, Std: {global_stats[3]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "XQZysej_lcO3",
        "outputId": "8396e961-3b1e-4cb1-f200-16ef52aa2120"
      },
      "outputs": [],
      "source": [
        "samples_list = [100, 600, 700]\n",
        "\n",
        "desired_samples = []\n",
        "\n",
        "# Loop through the indices and plot the waveform for each example\n",
        "for i, sample_idx in enumerate(samples_list):\n",
        "    x = X_train[sample_idx, :, 0]\n",
        "    y = Y_train[sample_idx, :, 0]\n",
        "    desired_samples.append((x, y))\n",
        "\n",
        "# Plot the extracted waveforms\n",
        "fig, axs = plt.subplots(nrows=1, ncols=len(desired_samples), figsize=(10, 4))\n",
        "for i, (x, y) in enumerate(desired_samples):\n",
        "    axs[i].plot(x.squeeze(), alpha=0.7)\n",
        "    axs[i].plot(y.squeeze(), alpha=0.7)\n",
        "    axs[i].set_title(f'Random Sample {i + 1}')\n",
        "    axs[i].set_xlabel('Time (samples)')\n",
        "    axs[i].set_ylabel('Amplitude')\n",
        "    axs[i].legend(['Input', 'Target'])\n",
        "    axs[i].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=2, ncols=len(desired_samples), figsize=(10, 6))\n",
        "\n",
        "# Setting sample rate and STFT parameters\n",
        "sample_rate = 16000\n",
        "frameSize = 512  # STFT window size\n",
        "hopSize = 256   # Overlap is frameSize - hopSize\n",
        "\n",
        "# Setting up essentia algorithms\n",
        "spectrum = es.Spectrum()\n",
        "windowing = es.Windowing(type='blackmanharris62', zeroPadding=frameSize)\n",
        "\n",
        "amp2db = es.UnaryOperator(type='lin2db', scale=1)\n",
        "pool = essentia.Pool()\n",
        "\n",
        "for idx, (x, y) in enumerate(desired_samples):\n",
        "    pool.clear()\n",
        "    x = x.squeeze().astype(np.float32)  # Convert data to float32\n",
        "    y = y.squeeze().astype(np.float32)\n",
        "\n",
        "\n",
        "    spectrograms_x = []\n",
        "    spectrograms_y = []\n",
        "\n",
        "    for frame_x, frame_y in zip(es.FrameGenerator(x, frameSize=frameSize, hopSize=hopSize),\n",
        "                                es.FrameGenerator(y, frameSize=frameSize, hopSize=hopSize)):\n",
        "        x_stft = spectrum(windowing(frame_x))\n",
        "        y_stft = spectrum(windowing(frame_y))\n",
        "        \n",
        "        pool.add('spectrum_db_x', amp2db(x_stft))\n",
        "        pool.add('spectrum_db_y', amp2db(y_stft))\n",
        "        \n",
        "\n",
        "    # Plotting the X spectrogram\n",
        "    img_x = axs[0, idx].imshow(pool['spectrum_db_x'].T, aspect='auto', origin='lower', interpolation='none', cmap='inferno')\n",
        "    axs[0, idx].set_title(f\"X - Spectrogram {idx + 1}\")\n",
        "    axs[0, idx].set_ylabel('Frequency (bins)')\n",
        "    axs[0, idx].set_xlabel('Time (frames)')\n",
        "    axs[0, idx].set_ylim(0, 500)\n",
        "    plt.colorbar(img_x, ax=axs[0, idx])  # Adding colorbar\n",
        "\n",
        "    # Plotting the Y spectrogram\n",
        "    img_y = axs[1, idx].imshow(pool['spectrum_db_y'].T, aspect='auto', origin='lower', interpolation='none', cmap='inferno')\n",
        "    axs[1, idx].set_title(f\"Y - Spectrogram {idx + 1}\")\n",
        "    axs[1, idx].set_ylabel('Frequency (bins)')\n",
        "    axs[1, idx].set_xlabel('Time (frames)')\n",
        "    axs[1, idx].set_ylim(0, 500)\n",
        "    plt.colorbar(img_y, ax=axs[1, idx])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import auraloss\n",
        "import torch \n",
        "import neural_audio_spring_reverb as nasr\n",
        "\n",
        "from neural_audio_spring_reverb.data.springset import load_springset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the dataset\n",
        "batch_size = 64\n",
        "train_loader, valid_loader, test_loader = load_springset(DATA_DIR, batch_size)\n",
        "\n",
        "# Initialize the ESR loss function\n",
        "esr_loss_func = auraloss.time.ESRLoss().to(device)\n",
        "mrstft_loss_func = auraloss.freq.MultiResolutionSTFTLoss().to(device)\n",
        "\n",
        "# Function to compute average ESR\n",
        "def compute_average_metrics(data_loader):\n",
        "    esr_total = 0.0\n",
        "    mrstft_total = 0.0\n",
        "    samples_count = 0\n",
        "\n",
        "    for dry, wet in data_loader:\n",
        "        dry, wet = dry.to(device), wet.to(device)\n",
        "        esr_value = esr_loss_func(dry, wet)\n",
        "        mrstft_value = mrstft_loss_func(dry, wet)\n",
        "        esr_total += esr_value.item() * dry.size(0)\n",
        "        mrstft_total += mrstft_value.item() * dry.size(0)\n",
        "        samples_count += dry.size(0)\n",
        "\n",
        "    average_esr = esr_total / samples_count\n",
        "    average_mrstft = mrstft_total / samples_count\n",
        "    return average_esr, average_mrstft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Naive Baseline\n",
        "\n",
        "# Compute average ESR for the desired dataset split\n",
        "avg_esr_naive, avg_mrstft_naive = compute_average_metrics(test_loader)\n",
        "print(\"Naive Baseline\")\n",
        "print(f\"Average ESR (Test Set): {avg_esr_naive}\")\n",
        "print(f\"Average MRSTFT (Test Set): {avg_mrstft_naive}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dummy Regressor\n",
        "\n",
        "class DummyRegressor(nn.Module):\n",
        "    def __init__(self, output_shape):\n",
        "        super(DummyRegressor, self).__init__()\n",
        "        self.output_shape = output_shape\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Generates a tensor with the same shape as the output, filled with random numbers\n",
        "        return torch.rand(self.output_shape).to(x.device)\n",
        "\n",
        "# [batch_size, 1, sample_length]\n",
        "\n",
        "dummy_regressor = DummyRegressor(output_shape=(batch_size, 1, 32000)).to(device)\n",
        "\n",
        "# Compute average ESR for the test set using the dummy regressor\n",
        "avg_esr_dummy, avg_mrstft_dummy = compute_average_metrics(test_loader)\n",
        "print(\"Dummy Regressor\")\n",
        "print(f\"Average ESR (Test Set with Dummy Regressor): {avg_esr_dummy}\")\n",
        "print(f\"Average MRSTFT (Test Set with Dummy Regressor): {avg_mrstft_dummy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
